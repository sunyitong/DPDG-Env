{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set adn freeze seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 105\n",
    "set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model's Architecture\n",
    "class DeepMetricEye(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepMetricEye, self).__init__()\n",
    "        # Downsample\n",
    "        # input 1*256*256\n",
    "        # Encoder 1\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # 64*256*256\n",
    "\n",
    "        # ----------------- #\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2)\n",
    "        # ----------------- #\n",
    "        # 64*128*128\n",
    "\n",
    "        # Encoder 2\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        # 128*128*128\n",
    "\n",
    "        # ----------------- #\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=2)\n",
    "        # ----------------- #\n",
    "        # 128*64*64\n",
    "\n",
    "        # Encoder 3\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        # 256*64*64\n",
    "\n",
    "        # ----------------- #\n",
    "        self.maxpool3 = nn.MaxPool2d(2, stride=2)\n",
    "        # ----------------- #\n",
    "        # 256*32*32\n",
    "\n",
    "        # Encoder 4\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.relu8 = nn.ReLU(inplace=True)\n",
    "        # 512*32*32\n",
    "\n",
    "        # ----------------- #\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2)\n",
    "        # ----------------- #\n",
    "        # 512*16*16\n",
    "\n",
    "        # Encoder 5\n",
    "        self.conv9 = nn.Conv2d(512, 1024, 3, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(1024)\n",
    "        self.relu9 = nn.ReLU(inplace=True)\n",
    "        self.conv10 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(1024)\n",
    "        self.relu10 = nn.ReLU(inplace=True)\n",
    "        self.dropout0 = nn.Dropout2d(p=0.5)\n",
    "        # 1024*16*16\n",
    "\n",
    "        # ----------------- #\n",
    "        self.maxpool5 = nn.MaxPool2d(2, stride=2)\n",
    "        # ----------------- #\n",
    "        # 1024*8*8\n",
    "\n",
    "        # Bottleneck\n",
    "        self.conv20 = nn.Conv2d(1024, 2048, 3, padding=1)\n",
    "        self.bn20 = nn.BatchNorm2d(2048)\n",
    "        self.relu20 = nn.ReLU(inplace=True)\n",
    "        self.conv21 = nn.Conv2d(2048, 2048, 3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(2048)\n",
    "        self.relu21 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        # 2048*8*8\n",
    "\n",
    "        # Upsample\n",
    "        # Decoder 5\n",
    "        # E3 256x64x64\n",
    "        self.D5_pool1 = nn.MaxPool2d(4, stride=4) # 256x16x16\n",
    "        self.D5_conv1 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.D5_bn1 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu1 = nn.ReLU(inplace=True) # 256x16x16\n",
    "\n",
    "        # E4 512x32x32\n",
    "        self.D5_pool2 = nn.MaxPool2d(2, stride=2) # 512x16x16\n",
    "        self.D5_conv2 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.D5_bn2 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu2 = nn.ReLU(inplace=True)\n",
    "        self.D5_conv3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.D5_bn3 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu3 = nn.ReLU(inplace=True) # 256x16x16\n",
    "\n",
    "        # E5 1024x16x16\n",
    "        self.D5_conv4 = nn.Conv2d(1024, 256, 3, padding=1) # 256x16x16\n",
    "        self.D5_bn4 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu4 = nn.ReLU(inplace=True)\n",
    "        self.D5_conv5 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.D5_bn5 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu5 = nn.ReLU(inplace=True) # 256x16x16\n",
    "\n",
    "        # Bottleneck 2048x8x8\n",
    "        self.D5_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # 2048*16*16\n",
    "        self.D5_conv6 = nn.Conv2d(2048, 256, 3, padding=1)\n",
    "        self.D5_bn6 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu6 = nn.ReLU(inplace=True)\n",
    "        self.D5_conv7 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.D5_bn7 = nn.BatchNorm2d(256)\n",
    "        self.D5_relu7 = nn.ReLU(inplace=True) # 256x16x16\n",
    "\n",
    "        # Concat\n",
    "        self.D5_conv8 = nn.Conv2d(256*4, 256*4, 3, padding=1)\n",
    "        self.D5_bn8 = nn.BatchNorm2d(256*4)\n",
    "        self.D5_relu8 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Decoder 4\n",
    "        # E3 256x64x64\n",
    "        self.D4_pool1 = nn.MaxPool2d(2, stride=2) # 256x32x32\n",
    "        self.D4_conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.D4_bn1 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu1 = nn.ReLU(inplace=True)\n",
    "        self.D4_conv2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.D4_bn2 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu2 = nn.ReLU(inplace=True) # 128x32x32\n",
    "\n",
    "        # E4 512x32x32\n",
    "        self.D4_conv3 = nn.Conv2d(512, 128, 3, padding=1) # 128x32x32\n",
    "        self.D4_bn3 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu3 = nn.ReLU(inplace=True)\n",
    "        self.D4_conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.D4_bn4 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu4 = nn.ReLU(inplace=True) # 128x32x32\n",
    "\n",
    "        # D5 1024x16x16\n",
    "        self.D4_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # 1024*32*32\n",
    "        self.D4_conv5 = nn.Conv2d(1024, 128, 3, padding=1)\n",
    "        self.D4_bn5 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu5 = nn.ReLU(inplace=True)\n",
    "        self.D4_conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.D4_bn6 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu6 = nn.ReLU(inplace=True) # 128x32x32\n",
    "\n",
    "        # Bottleneck 2048x8x8\n",
    "        self.D4_upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        # 2048*32*32\n",
    "        self.D4_conv7 = nn.Conv2d(2048, 128, 3, padding=1)\n",
    "        self.D4_bn7 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu7 = nn.ReLU(inplace=True)\n",
    "        self.D4_conv8 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.D4_bn8 = nn.BatchNorm2d(128)\n",
    "        self.D4_relu8 = nn.ReLU(inplace=True) # 128x32x32\n",
    "\n",
    "        # Concat\n",
    "        self.D4_conv9 = nn.Conv2d(128*4, 128*4, 3, padding=1)\n",
    "        self.D4_bn9 = nn.BatchNorm2d(128*4)\n",
    "        self.D4_relu9 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Decoder 3\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # 512*64*64\n",
    "        self.conv13 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.bn13 = nn.BatchNorm2d(256)\n",
    "        self.relu13 = nn.ReLU(inplace=True)\n",
    "        self.conv14 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn14 = nn.BatchNorm2d(256)\n",
    "        self.relu14 = nn.ReLU(inplace=True)\n",
    "        # 256*64*64\n",
    "\n",
    "        # Decoder 2\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # 256*128*128\n",
    "        self.conv15 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.bn15 = nn.BatchNorm2d(128)\n",
    "        self.relu15 = nn.ReLU(inplace=True)\n",
    "        self.conv16 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn16 = nn.BatchNorm2d(128)\n",
    "        self.relu16 = nn.ReLU(inplace=True)\n",
    "        # 128*128*128\n",
    "\n",
    "        # Decoder 1\n",
    "        self.upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # 128*256*256\n",
    "        self.conv17 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.bn17 = nn.BatchNorm2d(64)\n",
    "        self.relu17 = nn.ReLU(inplace=True)\n",
    "        self.conv18 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn18 = nn.BatchNorm2d(64)\n",
    "        self.relu18 = nn.ReLU(inplace=True)\n",
    "        # 64*256*256\n",
    "\n",
    "        self.conv19 = nn.Conv2d(64, 1, 1, padding=0)\n",
    "        # 1*256*256\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder 1\n",
    "        x1 = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x2 = self.relu2(self.bn2(self.conv2(x1)))\n",
    "\n",
    "        x3 = self.maxpool1(x2)\n",
    "\n",
    "        # Encoder 2\n",
    "        x3 = self.relu3(self.bn3(self.conv3(x3)))\n",
    "        x4 = self.relu4(self.bn4(self.conv4(x3)))\n",
    "\n",
    "        x5 = self.maxpool2(x4)\n",
    "\n",
    "        # Encoder 3\n",
    "        x5 = self.relu5(self.bn5(self.conv5(x5)))\n",
    "        x6 = self.relu6(self.bn6(self.conv6(x5)))\n",
    "\n",
    "        x7 = self.maxpool3(x6)\n",
    "\n",
    "        # Encoder 4\n",
    "        x7 = self.relu7(self.bn7(self.conv7(x7)))\n",
    "        x8 = self.relu8(self.bn8(self.conv8(x7)))\n",
    "\n",
    "        x9 = self.maxpool4(x8)\n",
    "\n",
    "        # Encoder 5\n",
    "        x9 = self.relu9(self.bn9(self.conv9(x9)))\n",
    "        x10 = self.relu10(self.bn10(self.conv10(x9)))\n",
    "        x10 = self.dropout0(x10)\n",
    "\n",
    "        x11 = self.maxpool5(x10)\n",
    "\n",
    "        # Bottleneck\n",
    "        x11 = self.relu20(self.bn20(self.conv20(x11)))\n",
    "        x12 = self.relu21(self.bn21(self.conv21(x11)))\n",
    "        x12 = self.dropout1(x12)\n",
    "\n",
    "        # Decoder 5\n",
    "        d5_e3 = self.D5_pool1(x6)\n",
    "        d5_e3 = self.D5_relu1(self.D5_bn1(self.D5_conv1(d5_e3)))\n",
    "\n",
    "        d5_e4 = self.D5_pool2(x8)\n",
    "        d5_e4 = self.D5_relu2(self.D5_bn2(self.D5_conv2(d5_e4)))\n",
    "        d5_e4 = self.D5_relu3(self.D5_bn3(self.D5_conv3(d5_e4)))\n",
    "\n",
    "        d5_e5 = self.D5_relu4(self.D5_bn4(self.D5_conv4(x10)))\n",
    "        d5_e5 = self.D5_relu5(self.D5_bn5(self.D5_conv5(d5_e5)))\n",
    "\n",
    "        d5_BN = self.D5_upsample1(x12)\n",
    "        d5_BN = self.D5_relu6(self.D5_bn6(self.D5_conv6(d5_BN)))\n",
    "        d5_BN = self.D5_relu7(self.D5_bn7(self.D5_conv7(d5_BN)))\n",
    "\n",
    "        d5 = torch.cat((d5_e3, d5_e4, d5_e5, d5_BN), 1)\n",
    "        d5 = self.D5_relu8(self.D5_bn8(self.D5_conv8(d5)))\n",
    "\n",
    "        # Decoder 4\n",
    "        d4_e3 = self.D4_pool1(x6)\n",
    "        d4_e3 = self.D4_relu1(self.D4_bn1(self.D4_conv1(d4_e3)))\n",
    "        d4_e3 = self.D4_relu2(self.D4_bn2(self.D4_conv2(d4_e3)))\n",
    "\n",
    "        d4_e4 = self.D4_relu3(self.D4_bn3(self.D4_conv3(x8)))\n",
    "        d4_e4 = self.D4_relu4(self.D4_bn4(self.D4_conv4(d4_e4)))\n",
    "\n",
    "        d4_e5 = self.D4_upsample1(d5)\n",
    "        d4_e5 = self.D4_relu5(self.D4_bn5(self.D4_conv5(d4_e5)))\n",
    "        d4_e5 = self.D4_relu6(self.D4_bn6(self.D4_conv6(d4_e5)))\n",
    "\n",
    "        d4_BN = self.D4_upsample2(x12)\n",
    "        d4_BN = self.D4_relu7(self.D4_bn7(self.D4_conv7(d4_BN)))\n",
    "        d4_BN = self.D4_relu8(self.D4_bn8(self.D4_conv8(d4_BN)))\n",
    "\n",
    "        d4 = torch.cat((d4_e3, d4_e4, d4_e5, d4_BN), 1)\n",
    "        d4 = self.D4_relu9(self.D4_bn9(self.D4_conv9(d4)))\n",
    "\n",
    "        # Decoder 3\n",
    "        x = self.upsample2(d4)\n",
    "        x = self.relu13(self.bn13(self.conv13(x)))\n",
    "        x = self.relu14(self.bn14(self.conv14(x)))\n",
    "\n",
    "        # Decoder 2\n",
    "        x = self.upsample3(x)\n",
    "        x = self.relu15(self.bn15(self.conv15(x)))\n",
    "        x = self.relu16(self.bn16(self.conv16(x)))\n",
    "\n",
    "        # Decoder 1\n",
    "        x = self.upsample4(x)\n",
    "        x = self.relu17(self.bn17(self.conv17(x)))\n",
    "        x = self.relu18(self.bn18(self.conv18(x)))\n",
    "\n",
    "        x = self.conv19(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "def load_data(data_dir):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = DepthDataset(data_dir, 'train', transform)\n",
    "    val_dataset = DepthDataset(data_dir, 'val', transform)\n",
    "    test_dataset = DepthDataset(data_dir, 'test', transform)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Creating DataLoader\n",
    "def create_dataloader(dataset, batch_size, shuffle=True, num_workers=0):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth Dataset\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, data_dir, split, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.depth_paths = []\n",
    "        with open(f'{data_dir}/{split}.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.image_paths.append(f'{data_dir}/{split}/rgb/{line}.png')\n",
    "                self.depth_paths.append(f'{data_dir}/{split}/depth/{line}.png')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        r, g, b = image.split()\n",
    "        image = b\n",
    "        depth = Image.open(self.depth_paths[index]).convert('L')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            depth = self.transform(depth)\n",
    "        return image, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                for i in tqdm(range(100)):\n",
    "                    clear_output(wait=True)\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        global  loss_save_name\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        loss_save_name = val_loss\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class ReverseHuberLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReverseHuberLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        abs_error = (pred - target).abs()\n",
    "        c = 0.25 * abs_error.max()\n",
    "        mask = abs_error <= c\n",
    "        loss = (mask.float() * abs_error ** 2 + (1 - mask.float()) * abs_error).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, train_dataloader, val_dataloader, epochs, lr, device, early_stopping_patience):\n",
    "    criterion = ReverseHuberLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in tqdm(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}')\n",
    "\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Test Function\n",
    "def test(model, test_dataloader, device):\n",
    "    criterion = ReverseHuberLoss()\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "        test_loss /= len(test_dataloader)\n",
    "        print(f'Test loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialise Model and Start Training\n",
    "data_dir = './train_data'\n",
    "batch_size = 30\n",
    "epochs = 150\n",
    "lr = 1e-4\n",
    "es_patience = 20\n",
    "loss_save_name = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = load_data(data_dir)\n",
    "\n",
    "train_dataloader = create_dataloader(train_dataset, batch_size)\n",
    "val_dataloader = create_dataloader(val_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "model = DeepMetricEye().to(device)\n",
    "\n",
    "train_loss_history, val_loss_history = train(model, train_dataloader, val_dataloader, epochs, lr, device, es_patience)\n",
    "\n",
    "test(model, test_dataloader, device)\n",
    "\n",
    "filename = f\"model_L4_test_bs{batch_size}_lr{lr}_epoch{epochs}_ReverseHuber_Loss{loss_save_name:.4f}_seed{seed}.pth\"\n",
    "save_path = \"./model/\" + filename\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"Model saved to %s\" % save_path)\n",
    "end_time = time.time()\n",
    "print(f'Time cost: {(end_time - start_time) / 60:.2f} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
